import re
import os
import json
import subprocess
import configparser
from glob import glob
from tempfile import TemporaryDirectory

import numpy as np
import pandas as pd

from .utility import dump_xyz, XYZ, ClusterOutput


ROOT = os.path.split(__file__)[0]
TCC_EXEC = os.path.abspath(f"{ROOT}/tcc")


class Parser:
    """
    A light-weight python parser for Topological Cluster Classification (TCC).

    It's main usage is to parse the result folder generated by TCC.

    TODO: Having the parser to remember the input parameters
    """
    def __init__(self, work_dir="", load_cache=True):
        """
        Args:
            work_dir (str): the path to the folder containing all tcc output\
                files. If the folder does not exist, it will be created when\
                `run` is called.
            load_cache (bool): if a cached file exists in the work_dir, then\
                use the cache to avoid repeated parsing.
        """
        if not work_dir:
            work_dir = "."
        self.__dir = work_dir
        self.__raw_dir = os.path.join(self.__dir, 'raw_output')
        self.__cluster_dir = os.path.join(self.__dir, 'cluster_output')
        cache_fn = os.path.join(self.__dir, 'pyTCC_cache.json')
        if load_cache and os.path.isfile(cache_fn):
            with open(cache_fn, 'rb') as f:
                cache = json.load(f)
            self.clusters_to_analyse = cache['clusters_to_analyse']
            self.cluster_bool = {
                name: XYZ.from_json(data_obj)
                for name, data_obj in cache['cluster_bool'].items()
            }
            self.cluster_detail = {
                name: XYZ.from_json(data_obj)
                for name, data_obj in cache['cluster_detail'].items()
            }
        else:
            self.clusters_to_analyse = []
            self.cluster_bool = {}    # if a particle is in different clusters
            self.cluster_detail = {}  # the particle indices of each cluster

    def __len__(self):
        """
        The frame number
        """
        if self.cluster_bool:  # not an empty cluster
            for key in self.cluster_bool:
                return len(self.cluster_bool[key])
        else:
            return 0

    def __write_box(self, box):
        """
        Generate a legal box.txt file in the tcc working directory
        """
        if box.ndim == 1:
            box = box[np.newaxis, :]  # (3,) --> (1, 3)
        if box.shape[1] != 3:
            raise RuntimeError("The dimension of box should be 3")
        box = np.concatenate((
            np.arange(box.shape[0])[:, np.newaxis],  # shape (n, 1)
            box  # shape (n, 3)
        ), axis=1)  # shape (n, 4)
        np.savetxt(
            fname=os.path.join(self.__dir, 'box.txt'),
            X=box,
            header='#iter Lx Ly Lz', fmt=["%d"] + ["%.8f"] * 3
        )

    def __write_parameters(self, **kwargs):
        """
        Write inputparameters.ini and clusters_to_analyse.ini

        A symlink named `sample` should be inside self__dir, linking
            to the xyz file to be analysed
        """
        input_parameters = {
            "Box": { "box_type": 1, "box_name": "box.txt" },
            "Run": { "xyzfilename": "sample", "frames": 1},
            "Simulation": {
                "rcutAA": 1.8, "rcutAB": 1.8, "rcutBB": 1.8, "min_cutAA": 0.0,
                "bond_type": 1, "PBCs": 1, "voronoi_parameter": 0.82,
                "num_bonds": 50, "cell_list": 1, "analyse_all_clusters": 1,
            },
            "Output": {
                "bonds": 0, "clusts": 1, "raw": 1, "do_XYZ": 0,
                "11a": 0, "13a": 0, "pop_per_frame": 1,
            }
        }

        clusters = { "Clusters": {
            "sp3a": 0, "sp3b": 0, "sp3c": 0, "sp4a": 0, "sp4b": 0, "sp4c": 0,
            "sp5a": 0, "sp5b": 0, "sp5c": 0, "6A": 0, "6Z": 0, "7K": 0,
            "7T_a": 0, "7T_s": 0, "8A": 0, "8B": 0, "8K": 0, "9A": 0, "9B": 0,
            "9K": 0, "10A": 0, "10B": 0, "10K": 0, "10W": 0, "11A": 0,
            "11B": 0, "11C": 0, "11E": 0, "11F": 0, "11W": 0, "12A": 0,
            "12B": 0, "12D": 0, "12E": 0, "12K": 0, "13A": 0, "13B": 0,
            "13K": 0, "FCC": 0, "HCP": 0, "BCC_9": 0, "BCC_15": 0,
            }
        }

        for key in kwargs:
            for section in input_parameters:
                if key in input_parameters[section].keys():
                    input_parameters[section][key] = kwargs[key]

        for key in clusters["Clusters"]:
            if key in self.clusters_to_analyse:
                clusters["Clusters"][key] = 1

        config_input = configparser.ConfigParser()
        config_input.read_dict(input_parameters)
        config_cluster = configparser.ConfigParser()
        config_cluster.read_dict(clusters)

        with open(
            os.path.join(self.__dir, "inputparameters.ini"), 'w'
        ) as f:
            config_input.write(f)
        with open(
            os.path.join(self.__dir, "clusters_to_analyse.ini"), 'w'
        ) as f:
            config_cluster.write(f)

    def run(self, xyz, box, frames=None, tcc_exec="", silent=True, **kwargs):
        """
        Call tcc to analyse an XYZ file. The output will be write to `self.__dir`

        Args:
            xyz (str): the path to the xyz file to be analysed. Notice that
                the working directory is `self.__dir` if using a relative path.
            box (list): the box of the simulation / experiment. The warpper
                supports different boxes in different frames.
            frames (int): the number of frames to perform TCC analysis. This
                wrapper DOES NOT check if this value is valid.
            tcc_exec (str): the path to TCC binary executable, if it is empty\
                the default tcc from extern/TCC will be used.
            silent (bool): if True the output of TCC will be supressed
            kwargs (dict): tcc parameters. These parameters will overwrite\
                the default parameters.

        Return:
            None
        """
        if not os.path.isfile(xyz):
            raise FileNotFoundError("The xyz file does not exist: ", xyz)
        if os.path.isabs(xyz):
            xyz_path = xyz
        else:
            xyz_path = os.path.abspath(xyz)

        if not tcc_exec:
            tcc_exec = TCC_EXEC  # use the default TCC executable

        if (self.__dir not in os.listdir(os.getcwd())) and (self.__dir != "."):
            os.makedirs(os.path.join(os.getcwd(), self.__dir))

        self.__write_box(np.array(box))

        # create a soft link of the xyz file to self.__dir
        soft_link = os.path.join(self.__dir, "sample")
        if os.path.isfile(soft_link):
            os.remove(soft_link)
        os.symlink(src=xyz_path, dst=soft_link)

        if isinstance(frames, type(None)):
            try:
                frames = len(XYZ(xyz_path, align_opt=True))
            except RuntimeError:
                frames = len(XYZ(xyz_path, align_opt=False))
        self.__write_parameters(frames=frames, **kwargs)
        if silent:
            subprocess.run(
                args=tcc_exec,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                cwd=self.__dir,
                check=True,
            )
        else:
            subprocess.run(args=tcc_exec, cwd=self.__dir, check=True)

    def __parse_raw(self):
        if not os.path.isdir(self.__raw_dir):
        #    raise FileNotFoundError(
        #        "No raw_output folder, rerun the tcc with Output.raw = 1"
        #    )
            return
        cluster_name_pattern = re.compile(r'sample.*raw_(.+)')
        filenames = glob(
            "{folder}/sample*raw_*".format(folder=self.__raw_dir)
        )
        filenames = [os.path.basename(fn) for fn in filenames]
        filenames.sort()
        cluster_names = [
            cluster_name_pattern.match(fn).group(1) for fn in filenames
        ]
        for cn in cluster_names:
            fn = glob(
                "{folder}/sample*raw_{cluster_name}".format(
                    folder=self.__raw_dir, cluster_name=cn
                )
            )
            if len(fn) == 0:
                raise FileNotFoundError(
                    "Raw output file for {c} not found".format(c=cn)
                )
            if len(fn) > 1:
                raise RuntimeError(
                    "Multiple raw file found for {c}".format(c=cn)
                )
            else:
                fn = fn[0]
            self.cluster_bool.update({
                cn: XYZ(fn, true_values=['C'], false_values=['A'], align_opt=True)
            })

    def __parse_cluster(self):
        if not os.path.isdir(self.__cluster_dir):
            #raise FileNotFoundError(
            #    "No cluster_output folder, rerun the tcc with Output.cluster = 1"
            #)
            return
        cluster_name_pattern = re.compile(r'sample.*clusts_(.+)')
        filenames = glob(
            "{folder}/sample*clusts_*".format(folder=self.__cluster_dir)
        )
        filenames = [os.path.basename(fn) for fn in filenames]
        filenames.sort()
        cluster_names = [
            cluster_name_pattern.match(fn).group(1) for fn in filenames
        ]
        for cn in cluster_names:
            fn = glob(
                "{folder}/sample*clusts_{cluster_name}".format(
                    folder=self.__cluster_dir, cluster_name=cn
                )
            )
            if len(fn) == 0:
                raise FileNotFoundError(
                    "Raw output file for {c} not found".format(c=cn)
                )
            if len(fn) > 1:
                raise RuntimeError(
                    "Multiple raw file found for {c}".format(c=cn)
                )
            else:
                fn = fn[0]
            self.cluster_detail.update({
                cn: ClusterOutput(fn, delimiter='\t')
            })

    def parse(self, raw=True, cluster=True, cache=True):
        if not os.path.isdir(self.__dir):
            raise FileNotFoundError(
                "Working directory does not exist\nUse .run() method to generate the results"
            )
        # TODO: parse the existing configuration files
        self.__parse_raw()
        self.__parse_cluster()
        if cache:
            self.__cache()

    def frame_bool(self, f, clusters=None):
        """
        Getting the result of particles and the clusters they are in. The
            output is a numpy array of Boolean values. One example would be

        ..code-block::

            id, FCC, 13A, 12E, 11F, 10B
            1,    0,   0,   0,   0,   1   # particle 1 is in 10B
            2,    1,   0,   0,   0,   0   # particle 2 is in FCC
            3,    0,   0,   0,   0,   0   # particle 3 is not in any cluster
            ...

        Args:
            f (int): the frame number
        """
        if isinstance(clusters, type(None)):
            clusters = self.cluster_bool.keys()
        result_dict = {}
        for cn in clusters:
            result_dict[cn] = self.cluster_bool[cn][f].ravel()
        return pd.DataFrame.from_dict(data=result_dict, orient='columns')

    def frame_count(self, f, clusters=None):
        """
        Getting the result of particles and the clusters they are in. The
            output is a numpy array of Boolean values. One example would be

        ..code-block::

            id, FCC, 13A, 10B, ...
            1,    2,   0,   0, ... # particle 1 is in 2 10B, 0 13A, and 0 10B
            2,    0,   1,   3, ... # particle 2 is in 0 FCC, 1 13A, and 3 10B
            ...

        Args:
            f (int): the frame number
        """
        if isinstance(clusters, type(None)):
            clusters = self.cluster_detail.keys()
        result_dict = {}

        for cn in clusters:
            n = self.cluster_bool[cn][f].shape[0]
            count = np.zeros(n, dtype=int)
            for cluster_indices in self.cluster_detail[cn][f]:
                count[cluster_indices] += 1
            result_dict[cn] = count

        return pd.DataFrame.from_dict(data=result_dict, orient='columns')

    @property
    def population(self):
        """
        Return the mean population if each frame as a pandas table

        Return:
            pandas.DataFrame: a pandas table where each columns are the clusters
                and each rows are different frames
        """
        pattern = 'sample*.pop_per_frame'
        fn = glob(os.path.join(self.__dir, pattern))
        if len(fn) == 1:
            fn = fn[0]
        elif len(fn) == 0:
            raise FileNotFoundError(
                "Population output file ({p}) not found".format(p=pattern)
            )
        else:
            raise RuntimeError("Multiple population output files exist")
        df = pd.read_csv(fn, sep='\t', header=0, index_col=0)
        df.dropna(axis='columns', inplace=True)
        return df.T

    def __cache(self):
        """
        Save the essential data to the hard disk, avoiding repeated parsing.
        """
        cache = {
            'cluster_bool': {
                name: data_obj.to_json()
                for name, data_obj in self.cluster_bool.items()
            },
            'cluster_detail': {
                name: data_obj.to_json()
                for name, data_obj in self.cluster_detail.items()
            },
            'clusters_to_analyse': self.clusters_to_analyse,
        }
        cache_fn = os.path.join(self.__dir, 'pyTCC_cache.json')
        with open(cache_fn, 'w') as f:
            json.dump(cache, f)


class OTF(Parser):
    """
    A light-weight python wrapper for TCC. The calculation is "on the fly", where

    Example:
        >>> ocf = tcc.OCF()
        >>> ocf(configurations, box=[10, 10, 10])
        >>> ocf.population
        >>> ocf.frame_bool
        >>> ocf.frame_count
    """
    def __init__(self):
        self.__tmp_dir = TemporaryDirectory()
        Parser.__init__(self, self.__tmp_dir.name)

    def run(self, configurations, box, tcc_exec="", silent=True, **kwargs):
        """
        Call tcc to analyse an XYZ file. The coordinates will be write to the hard disk
            temporarily on a random location, and the temp directory will be removed
            upon the destruction of the OTF object.

        Args:
            configurations (numpy.ndarray): the particle coordinaes in\
                different time points. The shape of the array should be\
                (n_frame, n_particle, 3)
            box (iterable): the box of the simulation / experiment. This\
                warpper supports different boxes in different frames.
            tcc_exec (str): the path to TCC binary executable, if it is empty\
                then the default tcc binary from tcclib will be used.
            silent (bool): if True the output of TCC will be supressed.
            kwargs (dict): tcc parameters. These parameters will overwrite\
                the default parameters.

        Return:
            None
        """
        if not tcc_exec:
            tcc_exec = TCC_EXEC  # use the default TCC executable

        xyz_name = os.path.join(self._Parser__dir, 'sample')
        for i, conf in enumerate(configurations):
            dump_xyz(xyz_name, conf, comment=i+1)

        self._Parser__write_box(np.array(box))
        self._Parser__write_parameters(frames=len(configurations), **kwargs)

        print(self._Parser__dir)

        if silent:
            subprocess.run(
                args=tcc_exec,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                cwd=self._Parser__dir,
                check=True,
            )
        else:
            subprocess.run(args=tcc_exec, cwd=self._Parser__dir, check=True)

    def __call__(self, configurations, box, tcc_exec="", silent=True, **kwargs):
        self.run(configurations, box, tcc_exec="", silent=True, **kwargs)
        self.parse()
